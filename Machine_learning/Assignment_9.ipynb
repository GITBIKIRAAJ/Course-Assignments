{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584ef392",
   "metadata": {},
   "source": [
    "Q1. **What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1838e10",
   "metadata": {},
   "source": [
    "**Feature engineering** is the process of designing and creating new features from existing data in order to improve the performance of a machine learning model. It is a critical step in the ***machine learning process***, as the quality and relevance of the features can have a significant impact on the model's ability to learn and make accurate predictions.\n",
    "\n",
    "_There are several aspects of feature engineering that can be considered:_\n",
    "\n",
    "`Data preprocessing`: This involves cleaning and preparing the raw data for analysis, such as filling in missing values, handling outliers, and removing irrelevant or redundant features.\n",
    "\n",
    "`Feature transformation`: This involves applying mathematical transformations to the features of the dataset in order to change their scale or distribution. Some common types of feature transformations include standardization, normalization, and log transformation.\n",
    "\n",
    "`Feature construction`: This involves creating new features from existing data by combining, aggregating, or summarizing existing features in a meaningful way. For example, a new feature could be created by taking the mean of a group of features, or by combining two features using a mathematical operation such as addition or multiplication.\n",
    "\n",
    "`Feature selection`: This involves selecting a subset of relevant features from the dataset to use in the model. The goal of feature selection is to reduce the dimensionality of the dataset and remove irrelevant or redundant features that do not contribute to the prediction task.\n",
    "\n",
    "`Feature extraction`: This involves using techniques such as dimensionality reduction or feature decomposition to extract important features from the dataset. These techniques can help to identify the underlying structure of the data and extract the most relevant features for the prediction task.\n",
    "\n",
    "Overall, the goal of feature engineering is to improve the performance of the machine learning model by selecting and constructing relevant and informative features that capture the patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf388166",
   "metadata": {},
   "source": [
    "Q2. **What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9286c",
   "metadata": {},
   "source": [
    "**Ans**: Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n",
    "\n",
    "There are **three types** of feature selection:\n",
    "\n",
    "1. Wrapper methods (forward, backward, and stepwise selection)\n",
    "2. Filter methods (ANOVA, Pearson correlation, variance thresholding)\n",
    "3. Embedded methods (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd256499",
   "metadata": {},
   "source": [
    "Q3. **Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de961c",
   "metadata": {},
   "source": [
    "**Ans**: The main differences between the filter and wrapper methods for feature selection are:\n",
    "\n",
    "1. Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "2. Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.\n",
    "3. Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use cross validation.\n",
    "4. Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.\n",
    "5. Using the subset of features from the wrapper methods make the model more prone to overfitting as compared to using subset of features from the filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f5bda",
   "metadata": {},
   "source": [
    "Q4. **Please Answer the following Questions :** <br>\n",
    "- Describe the overall feature selection process. <br>\n",
    "- Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281f2fa",
   "metadata": {},
   "source": [
    "**Ans**: \n",
    "Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model. There are three types of feature selection:\n",
    "\n",
    "**Wrapper methods** (forward, backward, and stepwise selection): In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "**Filter methods** (ANOVA, Pearson correlation, variance thresholding): Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here\n",
    "\n",
    "**Embedded methods** (Lasso, Ridge, Decision Tree): Embedded methods combine the qualities’ of filter and wrapper methods. It’s implemented by algorithms that have their own built-in feature selection methods. Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting. Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients. Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b57d39",
   "metadata": {},
   "source": [
    "Q5. **Describe the feature engineering process in the sense of a text categorization issue?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c8dcfa",
   "metadata": {},
   "source": [
    "In the context of a text categorization problem, feature engineering involves designing and creating features from the text data that can be used to predict the category of the text. Some common steps in the feature engineering process for a text categorization problem are:\n",
    "\n",
    "**Data preprocessing**: This involves cleaning and preparing the raw text data for analysis. This may include tasks such as lowercasing, stemming, and removing punctuation and stop words.\n",
    "\n",
    "**Feature transformation**: This may involve applying techniques such as term frequency-inverse document frequency (TF-IDF) to the text data in order to weight the importance of each term in the document relative to the entire corpus.\n",
    "\n",
    "**Feature construction**: This may involve creating new features from the text data by combining, aggregating, or summarizing existing features in a meaningful way. For example, a new feature could be created by taking the mean of the TF-IDF values of a group of terms, or by combining the presence or absence of certain terms using a binary operation.\n",
    "\n",
    "**Feature selection**: This may involve selecting a subset of relevant features from the dataset to use in the model. The goal of feature selection is to reduce the dimensionality of the dataset and remove irrelevant or redundant features that do not contribute to the prediction task.\n",
    "\n",
    "**Feature extraction**: This may involve using techniques such as latent semantic analysis or singular value decomposition to extract important features from the text data. These techniques can help to identify the underlying structure of the data and extract the most relevant features for the prediction task.\n",
    "\n",
    "Overall, the goal of feature engineering in a text categorization problem is to design and create relevant and informative features from the text data that can be used to predict the category of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68348374",
   "metadata": {},
   "source": [
    "Q6. **What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbd3e1",
   "metadata": {},
   "source": [
    "**Ans**: Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.\n",
    "\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space. It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes).\n",
    "\n",
    "The formula for calculating the cosine similarity is : **`Cos(x, y) = x . y / ||x|| * ||y||`**\n",
    "\n",
    "`In our Question cos(x,y) = 23/(root 40 * root 29) = 0.675`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c7983",
   "metadata": {},
   "source": [
    "Q7. **Explain the following:**\n",
    "1. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "2. Compare the Jaccard index and similarity matching coefficient of two features with values (1,1,0,0,1,0,1,1) and (1,1,0,0, 0,1,1,1), respectively (1,0,0,1,1,0,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249928c",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "1. The Hamming distance between two vectors is the number of bits we must change to change one into the other. Example Find the distance between the vectors 01101010 and 11011011. They differ in four places, so the Hamming distance `d(01101010,11011011) = 4`. In question mentioned between 10001011 and 11001111, hamming distance will be 2 as two character are different.\n",
    "\n",
    "2. Jaccard Index = (the number in both sets) / (the number in either set) * 100 For Question given, Jaccard Index = 2/2 *100 = 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18d57a",
   "metadata": {},
   "source": [
    "Q8.**State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8be2e",
   "metadata": {},
   "source": [
    "**Ans**:\n",
    "\n",
    "_High dimension_ is when variable numbers p is higher than the sample sizes n i.e. p>n, cases. High dimensional data is referred to a data of n samples with p features, where p is larger than n.\n",
    "\n",
    "For example, tomographic imaging data, ECG data, and MEG data. One example of high dimensional data is microarray gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a56bb2a",
   "metadata": {},
   "source": [
    "Q9. **Make a few quick notes on:**\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "2.  Use of vectors\n",
    "3.  Embedded technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696ec38",
   "metadata": {},
   "source": [
    "**Ans**:\n",
    "The **Principal component analysis (PCA)** is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data. The technique is widely used to emphasize variation and capture strong patterns in a data set.\n",
    "\n",
    "**Vectors** can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows\n",
    "\n",
    "In the context of machine learning, an **embedding techniques** is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfeaf1a",
   "metadata": {},
   "source": [
    "Q10. **Make a comparison between:**\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3e016",
   "metadata": {},
   "source": [
    "1. Sequential backward exclusion and sequential forward selection are two methods that can be used for feature selection, which is the process of selecting a subset of relevant features from a dataset to use in a machine learning model.\n",
    "\n",
    "Sequential backward exclusion (also known as backward selection) is a method of feature selection that starts with all of the features in the dataset and iteratively removes the least important features until the desired number of features is reached. The process of removing features is guided by a performance metric, such as accuracy or AUC, which is used to evaluate the performance of the model on the training data after each feature is removed. The features that have the greatest impact on the performance of the model are kept, while the least important features are removed.\n",
    "\n",
    "Sequential forward selection (also known as forward selection) is a method of feature selection that starts with an empty set of features and iteratively adds the most important features to the set until the desired number of features is reached. The process of adding features is also guided by a performance metric, which is used to evaluate the performance of the model on the training data after each feature is added. The features that have the greatest impact on the performance of the model are added to the set, while the least important features are ignored.\n",
    "\n",
    "In summary, sequential backward exclusion starts with all of the features in the dataset and iteratively removes the least important ones, while sequential forward selection starts with an empty set of features and iteratively adds the most important ones. Both methods aim to select a subset of relevant features that can improve the performance of the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136047f6",
   "metadata": {},
   "source": [
    "2. **The main differences between the filter and wrapper methods for feature selection are:**\n",
    "\n",
    "- Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "- Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.\n",
    "- Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use cross validation.\n",
    "- Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.\n",
    "- Using the subset of features from the wrapper methods make the model more prone to overfitting as compared to using subset of features from the filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc90d39",
   "metadata": {},
   "source": [
    "3. **SMC vs. Jaccard coefficient**\n",
    "\n",
    "SMC (short for \"simple matching coefficient\") and Jaccard coefficient are two measures that can be used to calculate the similarity between two sets.\n",
    "\n",
    "The simple matching coefficient (SMC) is a measure of the similarity between two sets that counts the number of elements that are shared by both sets and divides it by the total number of elements in both sets. The SMC can be calculated using the following formula:\n",
    "\n",
    "`SMC(A,B) = |A ∩ B| / |A ∪ B|`\n",
    "\n",
    "where A and B are the two sets, ∩ represents the intersection of the sets (i.e., the elements that are common to both sets), and ∪ represents the union of the sets (i.e., the elements that are in either set).\n",
    "\n",
    "The Jaccard coefficient (also known as the Jaccard index) is a measure of the similarity between two sets that counts the number of elements that are shared by both sets and divides it by the total number of elements in either set. The Jaccard coefficient can be calculated using the following formula:\n",
    "\n",
    "`Jaccard(A,B) = |A ∩ B| / |A ∪ B|`\n",
    "\n",
    "Both the SMC and Jaccard coefficient range from 0 to 1, where a value of 0 indicates that the sets have no elements in common, and a value of 1 indicates that the sets are identical.\n",
    "\n",
    "In summary, the SMC and Jaccard coefficient are measures of the similarity between two sets that count the number of elements that are shared by both sets and divide it by a measure of the total number of elements in the sets. The main difference between the two measures is in the denominator of the formula, which determines how the total number of elements in the sets is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b955b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
